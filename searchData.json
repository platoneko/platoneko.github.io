[{"title":"变分自编码器漫谈","url":"/2019/04/05/vae/","content":"# 从PCA说起\n\n主成分分析，或者称为PCA，是一种广泛使用的用来降低维度、抽取特征的技术。降维可以分为线性降维和非线性降维，PCA就是一种线性降维的方法，通过对原始特征做线性变换（由于加了偏置项 $b$ 其实是仿射变换），将数据投影到一个低维空间。那么应该做怎样的线性变换，投影到一个怎样的低维空间呢？PCA希望投影后的样本具有 **1.最近重构性：用样本经过投影变换后得到的新坐标来重构原始坐标，应该有最小的平方误差；2.最大可分性：投影后的样本点在新空间中应该尽可能分开，即使投影后的样本点的方差最大化。** 有趣的是，基于最近重构性和最大可分性推导，能分别得到同一组线性变换参数：\n$$\n\\begin{aligned}\n&\\min_W -tr(W^T X X^T W)\\\\\n&s.t. \\, W^T W = I\n\\end{aligned}\n$$\n\n$W$ 就是我们要求的线性变换，对输入样本 $x$ 进行降维即求 $z = W^T x$，$z$ 为中间隐变量，从 $z$ 重构 $x$ 即求 $\\hat{x} = W z = W W^T x$ 。\n\n<!-- more -->\n\n# 非生成式模型 -> 生成式模型\n\n前面介绍的PCA的形式是基于将数据线性投影到低维空间内。我们可以很容易地通过PCA计算输入数据 $x$ 在低维空间的表示 $z$ ，在实际应用中我们也更关注的是PCA的降维过程（编码），而不是重构过程（解码）。毕竟PCA的解码过程只能从已有的样本 $x$ 的隐变量 $z$ 去重构回 $x$ 自己，这根本就不具备生成功能，而我们现在想要一个能够生成新样本的模型。\n\n那么PCA能用来生成新样本吗？答案是可以的，PCA具备这样的潜能，但是需要从另一种概率角度来推导和理解PCA，才能把PCA纳入生成式模型。这种生成式的PCA被称作概率PCA。\n\n原始的PCA直接利用数据集 $X$ 的结构信息建模一个最优化问题，然后求解这个问题得到每个 $x_i$ 对应的 $z_i$ 。而概率PCA认为，数据集 $X$ 是对随机变量 $x$ 的若干次采样得到的，$x$ 依赖于随机变量 $z$ 。\n\n概率PCA定义隐变量 $z$ 的先验概率分布为标准高斯分布：\n\n$$\nz \\sim p(z) = \\mathcal{N} (z;0, I)\n$$\n\n接着定义观测变量 $x$ 的高斯条件概率分布 $p(x \\mid z)$ 。因为PCA中 $x$ 与 $z$ 的线性关系 $x = Wz + \\mu$，观测变量 $x$ 的条件概率分布还是高斯分布：\n\n$$\np(x \\mid z) = \\mathcal{N} (x; Wz + \\mu, \\sigma^2 I)\n$$\n\n有了上面两式，我们就可以首先为隐变量从标准高斯分布采样一个值 $z$，然后以这个隐变量为条件，对观测变量 $x$ 进行采样来生成新样本：\n\n$$\nx = Wz + \\mu + \\epsilon \\, ,\\epsilon \\sim \\mathcal{N} (0, \\sigma^2 I)\n$$\n\n那么现在的问题就是，如何确定参数 $W$, $\\mu$, $\\sigma$ 。我们可以使用极大似然的方式来确定参数，为了写出似然函数的表达式，我们需要观测变量的边缘概率分布的表达式：\n\n$$\np(x) = \\int p(x \\mid z) p(z)dz\n$$\n\n由于这对应于一个线性高斯模型，因此边缘分布还是高斯分布：\n\n$$\n\\mathbb{E}[x] = \\mathbb{E}[Wz + \\mu + \\epsilon] = \\mu\n$$\n\n$$\n\\begin{aligned}\ncov[x] &= \\mathbb{E}[(Wz + \\epsilon) (Wz + \\epsilon)^T]\\\\\n&= \\mathbb{E}[Wz z^T W^T] + \\mathbb{E}[\\epsilon \\epsilon^T]\\\\\n&= WW^T + \\sigma^2 I\n\\end{aligned}\n$$\n\n$$\np(x) = \\mathcal{N}(x;\\mu,WW^T + \\sigma^2 I)\n$$\n\n之后就可以通过最大化对数似然来确定模型的参数了。\n\n# 线性模型 -> 非线性模型\n\n前面的PCA以及概率PCA都是一种线性模型，它假设隐变量 $z$ 是观测变量 $x$ 的线性投影，这样的假设必然有它的局限性。而自编码器AutoEncoders则是一种非线性的降维模型。AutoEncoders的非线性在于，它在编码（降维）和解码（重构）的过程，都是用多层感知机，利用堆叠非线性激活函数来近似拟合任何函数。AutoEncoders是一种特殊的神经网络，他的输入和输出相同，训练目标是最小化重构的平方误差。可以说自编码器就是PCA的非线性版本，因此原始的自编码器也不具备生成式模型的功能。\n\n从之前的PCA到概率PCA，我们发现一个降维模型要升级到生成式模型，必须要有一个隐变量 $z$ 的先验分布，生成时要先对 $z$ 进行采样才能对新的样本 $x$ 进行重构。那我们再次将贝叶斯学派的概率图模型引入自编码器，于是就有了具备生成新样本能力的变分自编码器VAE了。\n\n将概率模型引入自编码器，和前面的概率PCA一样，我们想要最大化边缘概率分布 $p_\\theta (x) = \\int p_\\theta (z) p_\\theta (x \\mid z)dz$ 。但是和前面的概率PCA不同，我们这里的 $p(x \\mid z)$ 是通过神经网络拟合的复杂非线性函数，这就使得上面的积分式不再可积，也无法通过微分方法优化参数。而且后验分布 $p_\\theta (z \\mid x) = \\frac{p_\\theta (x \\mid z) p_\\theta (z)}{p_\\theta (x)}$ 因为分母不可积，也无法使用EM算法来优化参数。于是VAE的作者引入了变分推断：\n\n$$\n\\begin{aligned}\n\\log p_\\theta(x) =& \\log p_\\theta(x, z) - \\log p_\\theta(z \\mid x)\\\\\n=& \\log \\frac{p_\\theta(x, z)}{q_\\phi (z \\mid x)} - \\log \\frac{p_\\theta(z \\mid x)}{q_\\phi  (z \\mid x)}\\\\\n=& \\log p_\\theta(x, z) - \\log q_\\phi (z \\mid x) + \\log \\frac{q_\\phi  (z \\mid x)}{p_\\theta(z \\mid x)}\\\\\n\\end{aligned}\n$$\n\n等式两边同时对 $q_\\phi (z \\mid x)$ 求期望：\n\n$$\n\\begin{aligned}\n\\int q_\\phi (z \\mid x) \\log p_\\theta(x)dz =& \\int q_\\phi (z \\mid x) \\log p_\\theta(x, z)dz - \\int q_\\phi (z \\mid x) \\log q_\\phi (z \\mid x)dz\\\\ &+ \\int q_\\phi (z \\mid x) \\log \\frac{q_\\phi  (z \\mid x)}{p_\\theta(z \\mid x)}dz\\\\\n\\log p_\\theta(x) \\int q_\\phi (z \\mid x) dz =& \\mathbb{E}_{ {q_\\phi} (z \\mid x)}[\\log p_\\theta (x,z) - \\log q_\\phi (z \\mid x)] + KL(q_\\phi (z \\mid x) \\| p_\\theta (z \\mid x))\\\\\n\\log p_\\theta (x) =& KL(q_\\phi (z \\mid x) \\| p_\\theta (z \\mid x)) + \\mathcal{L}(\\theta , \\phi;x)\n\\end{aligned}\n$$\n\n由于KL散度的非负性，我们可以通过优化上式第二项变分下界来优化参数:\n\n$$\n\\begin{aligned}\n\\log p_\\theta (x) \\ge \\mathcal{L}(\\theta , \\phi;x) &= \\mathbb{E}_{ {q_\\phi} (z \\mid x)}[-\\log q_\\phi (z \\mid x) + \\log p_\\theta (x,z)]\\\\\n&= -KL(q_\\phi (z \\mid x) \\| p_\\theta (z)) + \\mathbb{E}_{ {q_\\phi} (z \\mid x)} [\\log p_\\theta (x\\mid z)]\n\\end{aligned}\n$$\n\n可见VAE并没有使用 $p(z)$ 是高斯分布的假设，而是假设后验分布 $p(z \\mid x)$ 是高斯分布。具体来说，在训练过程中，给定真实一个样本 $x$ ，我们假设存在一个专属于 $x$ 的分布 $p(z \\mid x)$ ，并假设这个分布是高斯分布。直观来看，因为我们后面要训练一个生成器 $X = g(Z)$，希望能够把从“专属”分布 $p(z \\mid x_k)$ 中采样出一个 $z_k$ 还原为 $x_k$。如果训练中，直接假设 $p(z)$ 是高斯分布并从中采样一个 $z$ ，我们就无法确定这个 $z$ 对应于哪个真实的 $x$。\n\n那如何找到后验分布 $p(z \\mid x)$ 呢？神经网络时代的哲学：遇事不决，神经网络！我们直接通过两个MLP把 $p(z \\mid x)$ 的均值和方差拟合出来，拟合成一个近似结果 $q_\\phi (z \\mid x)$ 。这个步骤其实也没那么粗暴，只不过是深度学习的基本操作，这不就是我们常用的自编码器的编码部分吗？\n\n有了这些，我们现在可以来关注一下目标函数的优化：\n\n$$\n\\mathcal{L}(\\theta , \\phi;x) = -KL(q_\\phi (z \\mid x) \\| p_\\theta (z)) + \\mathbb{E}_{ {q_\\phi} (z \\mid x)} [\\log p_\\theta (x\\mid z)]\n$$\n\n第一项是个KL散度。因为这是个生成式模型，我们用它生成的时候要先从一个先验分布 $p(z)$ 中采样出一个 $z$ 再输入解码器中来还原出 $x$ 来。这里干脆就定义这个先验分布为标准高斯分布。那么KL散度的计算结果就是：\n\n$$\n\\begin{aligned}\nKL(q_\\phi  (z \\mid x) \\| p_\\theta (z)) &= KL(\\mathcal{N}(\\mu, \\sigma^2 I) \\| \\mathcal{N}(0, I))\\\\\n&= \\frac{1}{2} \\sum_{i=1}^d (\\mu_{(i)}^2 + \\sigma_{(i)}^2 - \\log \\sigma_{(i)}^2 - 1)\n\\end{aligned}\n$$\n\n由于考虑的是各分量独立多元高斯分布，因此只需要推导一元高斯分布的情形即可：\n\n$$\n\\begin{aligned}&KL(N(\\mu,\\sigma^2)\\Vert N(0,1))\\\\\n=&\\int \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2} (\\log \\frac{e^{-(x-\\mu)^2/2\\sigma^2}/\\sqrt{2\\pi\\sigma^2}}{e^{-x^2/2}/\\sqrt{2\\pi}})dx\\\\\n=&\\int \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2} \\log \\{\\frac{1}{\\sqrt{\\sigma^2}}\\exp\\{\\frac{1}{2}[x^2-(x-\\mu)^2/\\sigma^2]\\} \\}dx\\\\\n=&\\frac{1}{2}\\int \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2} [-\\log \\sigma^2+x^2-(x-\\mu)^2/\\sigma^2] dx\\end{aligned}\n$$\n\n第一项是 $-log \\sigma^2$ 乘以概率密度的积分（相当于乘1），第二项是高斯分布二阶矩 $\\mu^2 + \\sigma^2$，第三项是\"-方差/方差\"结果就是-1。\n\n接下来就是目标函数的第二项，第二项对应于解码重构的过程。$\\mathbb{E}_{ {q_\\phi} (z \\mid x)} [\\log p_\\theta (x\\mid z)]$ 就等价于 $\\frac{1}{n} \\sum^N_i \\log p_\\theta (x\\mid z_i) \\,\\, , z_i \\sim {q_\\phi} (z \\mid x)$，我们只需从编码过程计算出的后验高斯分布中采样出一个 $z$ ，然后去计算 $\\log p_\\theta (x\\mid z)$ 即可。但是这里看上去好像有点问题：采样一个够吗？中山大学苏神：“事实上我们会运行多个epoch，每次的隐变量都是随机生成的，因此当epoch数足够多时，事实上是可以保证采样的充分性的。我也实验过采样多个的情形，感觉生成的样本并没有明显变化。”那就这样吧。\n\n那生成模型中的条件分布 $p_\\theta (x\\mid z)$ 是啥呢？论文中给出了两种分布：伯努利分布（用于二值的 $x$）和高斯分布（用于连续实数的 $x$）。\n\n伯努利分布模型：\n\n$$\nq(x|z)=\\prod_{k=1}^D (\\rho_{(k)}(z))^{x_{(k)}} (1 - \\rho_{(k)}(z))^{1 - x_{(k)}}\n$$\n\n$$\n-\\ln q(x|z) = \\sum_{k=1}^D [- x_{(k)} \\ln \\rho_{(k)}(z) - (1-x_{(k)}) \\ln (1 -\\rho_{(k)}(z))]\n$$\n\n高斯分布模型：\n\n$$\nq(x|z)=\\frac{1}{\\prod\\limits_{k=1}^D \\sqrt{2\\pi  \\sigma_{(k)}^2(z)}}\\exp(-\\frac{1}{2}\\Vert\\frac{x-\\mu(z)}{\\sigma(z)}\\Vert^2)\n$$\n\n$$\n-\\ln q(x|z) = \\frac{1}{2}\\Vert\\frac{x-\\mu(z)}{\\sigma(z)}\\Vert^2 + \\frac{D}{2}\\ln 2\\pi + \\frac{1}{2}\\sum_{k=1}^D \\ln \\sigma_{(k)}^2(z)\n$$\n\n但是我们重构 $x$ 时并不会真的从这个高斯分布中采样，而是直接以高斯分布概率密度最大点（即均值）作为重构结果，因此我们用不上方差 $\\sigma^2$。因为如果是采样出 $x$，那模型就无法通过微分方法（梯度下降）优化参数了。这时候：\n\n$$\n-\\ln q(x|z) \\sim \\frac{1}{2\\sigma^2}\\Vert x-\\mu(z)\\Vert^2\n$$\n\n可以发现，其形式就是一个MSE损失。\n\n可是现在还有一个问题：如果 $z$ 是通过预测后验分布 $q_\\phi (z \\mid x)$ 采样取得，采样过程是不可微的，那我们要怎样才能优化编码器部分的参数呢？这里，VAE使用了一个重参数技巧：从 $\\mathcal{N}(\\mu, \\sigma^2)$ 中采样一个 $z$ 可以等价于从 $\\mathcal{N}(0,I)$ 中采样一个 $\\varepsilon$ 使 $z = \\mu + \\varepsilon \\times \\sigma$ 。这样一来，“采样”这个操作就不用参与梯度下降了，改为采样的结果参与，使得整个模型可训练了。整个VAE框架的流程就完成了。\n\n# 如果直面联合分布？\n\n考虑从另一角度推导VAE的目标函数：干脆直接来对 $p(x,z)$ 做近似。定义 $p(x,z)=\\tilde{p}(x)p(z|x)$，设想用联合分布 $q(x,z)$ 来近似 $p(x,z)$：\n\n$$\nKL(p(x,z)\\Vert q(x,z)) = \\iint p(x,z)\\ln \\frac{p(x,z)}{q(x,z)} dzdx\n$$\n\n$$\n\\begin{aligned}KL(p(x,z)\\Vert q(x,z)) =& \\int \\tilde{p}(x) [\\int p(z|x)\\ln \\frac{\\tilde{p}(x)p(z|x)}{q(x,z)} dz]dx\\\\ \n=& \\mathbb{E}_{x\\sim \\tilde{p}(x)} [\\int p(z|x)\\ln \\frac{\\tilde{p}(x)p(z|x)}{q(x,z)} dz] \n\\end{aligned}\n$$\n\n$$\n\\ln \\frac{\\tilde{p}(x)p(z|x)}{q(x,z)}=\\ln \\tilde{p}(x) + \\ln \\frac{p(z|x)}{q(x,z)}\n$$\n\n$$\n\\begin{aligned}\\mathbb{E}_{x\\sim \\tilde{p}(x)} [\\int p(z|x)\\ln \\tilde{p}(x)dz] =& \\mathbb{E}_{x\\sim \\tilde{p}(x)} [\\ln \\tilde{p}(x)\\int p(z|x)dz]\\\\ \n=&\\mathbb{E}_{x\\sim \\tilde{p}(x)} [\\ln \\tilde{p}(x)] \n\\end{aligned}\n$$\n\n注意到 $\\mathbb{E}_{x\\sim \\tilde{p}(x)} [\\ln \\tilde{p}(x)]$ 是一个常数项，因此目标函数可以简化为：\n\n$$\n\\begin{aligned}\\mathcal{L} =& \\mathbb{E}_{x\\sim \\tilde{p}(x)} [\\int p(z|x)\\ln \\frac{p(z|x)}{q(x,z)} dz]\\\\\n=& \\mathbb{E}_{x\\sim \\tilde{p}(x)} [\\int p(z|x)\\ln \\frac{p(z|x)}{q(x|z)q(z)} dz]\\\\ \n=&\\mathbb{E}_{x\\sim \\tilde{p}(x)} [-\\int p(z|x)\\ln q(x|z)dz+\\int p(z|x)\\ln \\frac{p(z|x)}{q(z)}dz]\\end{aligned}\n$$\n\n最终形式：\n\n$$\n\\begin{aligned}\\mathcal{L} = &\\mathbb{E}_{x\\sim \\tilde{p}(x)} [\\mathbb{E}_{z\\sim p(z|x)}[-\\ln q(x|z)]+\\mathbb{E}_{z\\sim p(z|x)}[\\ln \\frac{p(z|x)}{q(z)}]]\\\\\n= &\\mathbb{E}_{x\\sim \\tilde{p}(x)} [\\mathbb{E}_{z\\sim p(z|x)}[-\\ln q(x|z)]+KL(p(z|x)\\Vert q(z))]\n\\end{aligned}\n$$\n\n这正是VAE的损失函数，我们直面联合分布，中间没有对后验分布进行分析，居然直接就得到了这个结果。\n\n# VAE损失函数的直观理解\n\n损失函数中，最为直观的就是重构损失，它要求重构后的 $X$ 和原始的 $X$ 尽可能接近。如果仅仅只有重构损失，那么VAE就算前面做了再多努力，又是拟合均值方差又是后验采样，其最终还是会退化成一个普通的自编码器，失去了生成的功能。虽然 $z$ 是通过采样得到的，采样就会有噪声，显然噪声会增加重构的难度，不过好在这个噪声强度（也就是方差）通过一个神经网络算出来的，所以最终模型为了重构得更好，肯定会想尽办法让方差为0。而方差为0的话，也就没有随机性了，所以不管怎么采样其实都只是得到确定的结果（也就是均值）。这时，KL散度的作用就显现了，它要求所有的 $p(X \\mid Z)$ 都要向标准高斯分布靠拢，这样就防止了噪声为0，同时保证了生成过程能放心地从标准高斯分布中采样一个 $z$ 来产生新样本：\n\n$$\np(Z)=\\sum_X p(Z|X)p(X)=\\sum_X \\mathcal{N}(0,I)p(X)=\\mathcal{N}(0,I) \\sum_X p(X) = \\mathcal{N}(0,I)\n$$\n\n而且，KL损失和重构损失还存在对抗关系。直觉上来想，当decoder还没有训练好时（重构误差远大于KL loss），就会适当降低噪声（KL loss增加），使得拟合起来容易一些（重构误差开始下降）；反之，如果decoder训练得还不错时（重构误差小于KL loss），这时候噪声就会增加（KL loss减少），使得拟合更加困难了（重构误差又开始增加），这时候decoder就要想办法提高它的生成能力了。（浑然天成！）\n","tags":["deep learning","machine learning"],"categories":["deep learning"]},{"title":"关于交叉熵的一些理解","url":"/2019/03/21/cross-entropy/","content":"接触机器学习和深度学习已经有一年多时间,曾经自以为已经对交叉熵的概念已经很熟悉了,日常涉猎的都是一些更高层面的针对特定问题的DL/ML算法。然而最近研究多轮对话接触到vhred以及其他变分模型的时候,才发现自己对交叉熵、KL散度的概念并不够清楚,甚至还存在一些误解。借此机会重新整理一下交叉熵的内容。（*以下论述主要是一些通俗理解，追求数学严谨的同学请移步*）\n\n交叉熵是信息论中的一个概念，要想了解交叉熵的本质，就需要从最基本的概念说起。\n\n# 1.信息量\n\n假设 $X$ 是一个离散型随机变量，其取值集合为 $\\chi$，概率分布函数 $p(x)=P(X=x), x \\in \\chi$，则定义事件 $X=x_0$ 的信息量为：\n\n$$I(x_0)=- \\log (p(x_0))$$\n\n直观来看，信息量和事件发生的概率有关，越不可能发生的事件发生了，我们获取到的信息量就越大，而那些司空见惯的事发生了，我们获取到的信息量就很少，**即信息量可以表示一个事件的自信息量**。从另一角度看，信息量也可以衡量一个事件的不确定度，也就是说我们需要加入多少信息才能将该事件的发生确定下来，**事件的不确定性越大，信息量也就越大，把它搞清楚所需要的信息量也就越大**。\n\n# 2.信息熵\n\n信息熵是随机变量的信息量的期望：\n\n$$H(X) = - {\\sum}_{i=1}^n p(x_i) \\log (p(x_i))$$\n\n广义的熵可以衡量一个系统的有序化程度（或混乱程度），可以证明，一个系统越是混乱不确定（均匀分布），熵就越大。信息熵用于衡量数据的信息量时，可以得到对数据进行编码所需的最短平均编码长度，当平均编码长度大于信息熵时可以认为编码存在冗余。\n\n# 3. KL散度（相对熵）\n\n如果我们对于同一个随机变量 $X$ 有两个单独的概率分布 $P(X)$ 和 $Q(X)$，我们可以使用KL散度来衡量这两个分布的差异（**不是距离，因为不具备对称性！**）\n\n$$KL(p \\mid \\mid q)= {\\sum}_{i=1}^n p(x_i) \\log (\\frac{p(x_i)}{q(x_i)})$$\n\n在机器学习中，$KL(P \\mid \\mid Q)$ 通常用来描述使用真实分布 $P$ 取代估计分布 $Q$ 时的信息增益。\n\nKL散度的值越小，表示 $P$ 分布和 $Q$ 分布越接近。\n\n（这里还有对KL散度的另一种解读：KL散度的意义是“额外所需的编码长度”如果我们用B的编码来表示A）\n\n# 4. 交叉熵\n\n直接给出公式：\n\n$$\\begin{aligned}\nH(p, q) &= {\\sum}_{i=1}^n p(x_i) \\log (\\frac{1}{q(x_i)})\\\\\n        &= - {\\sum}_{i=1}^n p(x_i) \\log q(x_i)\n\\end{aligned}$$\n\n可以发现 $KL(p \\mid \\mid q) = H(p,q)-H(p)$，在机器学习中，我们需要评估**某个样本**的label和predicts之间的差距，使用KL散度刚刚好，由于KL散度中的 $H(p)$ 一直保持不变，故优化中只需要关注交叉熵即可。\n\n重点开始：深度学习/机器学习中大部分任务都可以看作分类任务，我们这里说的分布 $p$ 和 分布 $q$ 是针对对数据集中的**某一个**样本 $x$ 的label而言，而不是整个数据集中的所有样本的label分布。 我们的任务是训练一个函数，向函数中输入一个样本 $x$ 可以得到其label的**概率分布列** $\\vec{P}(Y=y \\mid x)$，训练的目标是对每个已观测到的样本 $(x,y)$，通过这个函数计算出的label分布列都要和观测的真实分布列尽可能接近。对于每个样本 $x$ 只有一种确定的label的情况（通常情况如此），其真实概率分布列 $p$ 其实是一个形如 $[0,1,0,0,0]$ 的向量，其信息熵 $H(p)$ 通过公式计算的结果为0（这也印证了对于已观测到的样本，其label的概率分布已知，不确定度为0），可以认为深度学习/机器学习中对常见分类任务使用的交叉熵其实就是”退化“的KL散度；由于样本真实概率分布 $p$ 为形如 $[0,1,0,0,0]$ 的向量，交叉熵 $H(p,q)$ 也只剩下真实概率分布中数值为1对应的项，此时交叉熵就完全等价于极大对数似然估计了，其优化目标是最大化已观测样本的 $(x,y)$ 的出现概率。\n","tags":["deep learning","machine learning"],"categories":["deep learning"]},{"title":"基于知识的对话系统","url":"/2019/02/28/knowledge-based-text-generation/","content":"开始涉猎基于知识的文本生成任务（Text Generation），重点主要在外部知识的嵌入（embedding）和编码（encoding）以及如何将编码后的知识集成到文本生成任务当中。其中的知识一般是三元组形式（triplet）也可以是非结构化的自然语言文本;所涉及的生成任务包括对话生成、生成式问答系统、故事结尾推测及补全等。-后续可能还会更新-\n\n# **Introduction**\n\n早期的基于神经网络的文本生成任务大多是纯数据驱动的基于Seq2Seq框架的（暴力）模型，甚至是直接将NMT任务的Encoder-Decoder框架搬过来，仅仅是换了个数据集进行调参（如今这种灌水肯定是不行了）。这样直接生搬硬套的模型虽然确实产生了一些有意思的结果，生成的文本看上去很接近自然语言，但是过于ambiguous往往缺乏有实质信息(informative)的内容。我们就从直觉出发，显然一个正常人在对话和写作过程会加入个人的经验、常识等超出上下文内容的信息，因此将外界知识引入是必不可少的。无论是对自然语言的理解（NLU）还是生成自然语言（NLG），知识的引入都肯定会使系统效果有显著提升。从训练数据上来看，我们要完成从对上下文的理解、推理最后到文本的生成，上下文中出现的一些实体（entity）隐含的关系、性质等信息往往要借助外界知识才能完善，这些关系性质仅仅从训练数据集中是无法获取和推理的，直接去拟合上下文的匹配其结果往往只能使系统学习到语法结构和一些最简单最基本的标准 response。\n\n<br>\n\n# **Augmenting End-to-End Dialogue System with Commonsense Knowledge**\n<https://arxiv.org/pdf/1709.05453.pdf>\n\n> **Motivation**\n\n在人类对话中，人们不仅仅只关注于对话内容本身，还要将对话中各种概念的相关信息集成在回复中。由于常识知识（commonsense knowledge）可以说是海量的，作者认为运用一个包含常识的外部记忆模块比传统方法中强迫系统去将这些海量的知识编码在模型参数中更为可靠。在论文中作者测试了用常识知识来增强End2End的对话系统。\n\n> **Model**\n\n论文的数据集是形如 <$message$, $response$, $label$> 的三元组，$label$ 用来标识 $response$ 是否和 $message$ 匹配，目标模型其实是一个二分类模型，因此我们主要关注 commonsense knowledge 的 embedding 和 encoding。\n\n论文的commonsense knowledge采用通过ConceptNet获取的三元组 <$concept1$, $relation$, $concept2$> 作为assertion并假设commonsense knowledge base由大量关于concepts $C$ 的 assertions $A$ 组成。值得注意的是，concept $c$ 可以是单个单词也可以是多个单词。作者构建了一个以 $c$ 作为key，以所有包含 $c$（可以是 $concept1$ 或 $concept2$）的assertions作为value的字典。设 $A_x$ 为message $x$ 中所有相关的assertions的集合，作者通过n-gram matching的方法将match到的concept对应的所有assertions加入到 $A_x$。\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f1.png\"></div>\n\n论文使用LSTM来encoding  $A_x$ 中的所有assertions。每个形如 <$c_1$, $r$, $c_2$> 的 $a$ 都被视为一个tokens序列，e.g.对于multi-word concept $c_1$, $c_2$, $a$ = $[c_{11}, c_{12}, c_{13}..., r, c_{21}, c_{22}, c_{23}...]$ 。作者将concept和relation中的词当作常规词一并加入vocabulary $V$ 中进行embedding。\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f2.png\"></div>\n\n我们不详细讨论模型后面的task-orient部分。但是从上图可以看到，作者没有显式计算assertions $a$ 与message $x$ 的相关性score，而是分别计算response $y$ 与 message $x$ 和 assertions $a$ 的score。可以认为assertions $a$ 的集成，**其本质就是给原模型加入了一个response要与message中出现的concept1的通过某种relation关联的concept2相关的先验偏好。**（个人认为在对message进行encoding的过程中也有必要加入commonsense knowledge，因为前面的词的相关concept可能对后面的词的语义产生影响。）\n\n之后作者还提出了一个变种：\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f3.png\"></div>\n\n其中 $p_i$ 是 $x$ 对 $a_i$ 的dot-product attention score，使与message context相关性高的assertion具有更大的权值。作者在这里把 $A_x$ 视作Memory Networks的memory component。\n\n<br>\n\n# Commonsense Knowledge Aware Conversation Generation with Graph Attention\n\n<https://www.ijcai.org/proceedings/2018/0643.pdf>\n\n（Paperweekly上正好有对这篇paper的解读，很多内容直接搬运了）\n\n> **Motivation**\n\n在以前的工作中，对话生成的信息源是文本与对话记录。但是这样一来，如果遇到OOV的词，模型往往难以生成合适的、有信息量的回复，而会产生一些低质量的、模棱两可的回复，这种回复往往质量不高。\n\n为了解决这个问题，有一些利用常识知识图谱生成对话的模型被陆续提出。当使用常识性知识图谱时，由于具备背景知识，模型更可能理解用户的输入，这样就能生成更加合适的回复。但是，这些结合了文本、对话记录、常识知识图谱的方法，往往只使用了单一三元组，而忽略了一个子图的整体语义，会导致得到的信息不够丰富。\n\n为了解决这些问题，文章提出了一种**基于常识知识图谱的对话模型**（commonsense knowledge aware conversational model, CCM）来理解对话，并且产生信息丰富且合适的回复。\n\n**本文提出的方法利用了大规模的常识性知识图谱。** 首先是理解用户请求，找到可能相关的知识图谱子图；再利用静态图注意力（static graph attention）机制，结合子图来理解用户请求；最后使用动态图注意力（dynamic graph attention）机制来读取子图，并产生合适回复。\n\n通过这样的方法，本文提出的模型可以生成合适的、有丰富信息的对话，提高对话系统的质量。\n\n> **Model**\n\n**1. CCM模型**\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f4.png\"></div>\n\n如图所示，基于n个词的输入，会输出m个词作为回复，模型的目的就是预估这么一个概率分布 $P(Y|X,G)=\\Pi^m_{t=1}P(y_t|y_{<t},X,G)$，即将图信息 $G$ 加入到概率分布的计算中。\n\n在信息读取时，根据每个输入的词 $x$，找到常识知识图谱中对应的子图（若没有对应的子图，则会生成一个特殊的图 *Not_A_Fact*），每个子图又包含若干三元组。\n\n**2. 知识编译模块（Knowledge Interpreter）**\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f5.png\"></div>\n\n如图所示，当编译到\"rays\"时，会把这个词在知识图谱中相关的子图得到，并生成子图向量。每一个子图都包含了key entity（即这里的\"rays\"），以及这个\"rays\"的邻居实体和相连关系。\n\n对于词\"of\"，由于无法找到对应子图，所以就采用特殊子图 *Not_A_Fact* 来编译。之后采用基于静态注意力机制，CCM会将子图映射为向量，然后把词向量 $w(x_t)$ 和 $g_i$ 拼接作为encoder中GRU的输入。\n\n对于静态图注意力机制，CCM将子图中所有三元组 $k_n=(h_n, r_n, t_n)$ 考虑进来：\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f6.png\"></div>\n\n其中三元组 $k_n=(h_n, r_n, t_n)$ 选用**TranE**进行embedding。\n\n**3. 知识生成模块（Knowledge Aware Generator）**\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f7.png\"></div>\n\n在生成时，不同于静态图注意力机制，模型会读取所有相关的子图，而不是当前词对应的子图，并计算decoder state $s_t$ 时使用每一个子图 $g_i$ 的概率：\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f8.png\"></div>\n\n之后模型会计算选择decoder state $s_t$ 时每个子图 $g_i$ 中选择一个三元组 $k_j$ 来生成单词的概率：\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f9.png\"></div>\n\n生成时，会根据计算结果，来选择是生成通用字（generic word）还是子图中的实体：\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f10.png\"></div>\n\n**4. 损失函数**\n\n损失函数为预期输出与实际输出的交叉熵，除此之外，为了监控选择通用词还是实体的概率，又增加了一个交叉熵：\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f11.png\"></div>\n\n> **Experiment**\n\n**实验相关细节**\n\n常识性知识图谱选用了ConceptNet，对话数据集选用了reddit的10M single-round 对话数据集，如果一个post-response不能以一个三元组表示（一个实体出现于post，另一个出现于response）,就将这个数据去除（filter the original corpus with the knowledge triplets）。\n\n然后对剩下的对话数据分为四类，一类是高频词，即每一条post的每一个词，都是最高频的25%的词；一类是中频词即25%-75%；一类是低频词即高于75%；最后一类是OOV词，每一条post包含有OOV词。\n\n基线系统选择了如下三个：只从对话数据中生成response的seq2seq模型、存储了以TranE形式表示知识图谱的MemNet模型[[Ghazvininejad et al., 2017]](https://arxiv.org/pdf/1702.01932.pdf)、从三元组中copy一个词或生成通用词的CopyNet模型[[Zhu et al., 2017]](https://arxiv.org/pdf/1709.04264.pdf)。\n\n而选用metric的时候，采用了刻画回复内容是否语法正确且贴近主题的perplexity，以及有多少知识图谱实体被生成的entity score。\n\n**实验结果**\n\n如下图所示为根据perplexity和entity score进行的性能比较，可见CCM的perplexity最低，且选取entity的数量最多。并且，在低频词时，选用的entity更多，这表示在训练时比较罕见的词（实体）会需要更多的背景知识来生成答复。\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f12.png\"></div>\n\n另外，作者还采用crowdsourcing的方式来人为审核response的质量，并采用了两种度量值：appropriateness（内容是否语法正确，是否与主题相关，是否有逻辑）和 informativeness（内容是否提供了post之外的新信息）。\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f13.png\"></div>\n\n从上图可见，CCM对于三个基线系统来说，都有将近60%的回复是更优的。并且在OOV数据集上，CCM比seq2seq不知道高到哪里去了，这是由于CCM对于这些低频词或未登录词，可以用知识图谱去补全，而seq2seq没有这样的知识来源。\n\n如下图所示，当在post中遇到未登录词\"breakable\"时，seq2seq和MemNet都只能输出一些通用的、模棱两可的、毫无信息量的回复。CopyNet能够利用知识图谱输出一些东西，但是并不合适。而CCM却可以输出一个合理的回复。\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f14.png\"></div>\n\n<br>\n\n# Knowledge Diffusion for Neural Dialogue Generation\n\n<http://aclweb.org/anthology/P18-1138>\n\n> **Motivation**\n\n和前面论文的描述类似，对话系统需要加入common knowledge，就不再赘述。论文的不同之处在于，作者认为对话系统应该具备通过当前会话中包含的facts发散到基于知识图谱的相似实体上：\n\n1. ***facts matching***： 尽管有些对话是与facts相关的询问（inquiries），即主体和关系可以很容易被识别出来，但是对于某些对话，主体和关系是难以捉摸的，给准确的facts matching造成麻烦。下图展示了一个示例：Item 1 和 2 都在谈论电影\"Titanic\"，其中item 1是个典型的问答式对话，而item 2则是一个知识相关的没有任何显式关系的聊天式对话（chit-chat）。在item 2中去定义准确的fact match是非常困难的。\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f15.png\"></div>\n\n1. ***entity diffusion***：另一个显著的现象是，对话常常会从一个实体转移到另一个实体。如item 3 和 4 都和实体\"Titanic\"有关，然而回复中的实体都是其他的类似的电影。像这样的发散式关系，能够在现有的知识图谱三元组中去捕捉到它们的情况是十分罕见的。\n\n因此，论文提出了一个神经知识发散（Neural Knowledge Diffusion, NKD）对话系统。NKD先去匹配对话与相关的事实（facts），之后匹配到的事实会用来发散到相似的实体，最后通过获取到的所有知识项来生成回复。\n\n> **Model**\n\n**Overview**\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f16.png\"></div>\n\n论文构建了一个多轮对话（Multi-turn）模型。模型由四部分组成：\n\n 1. 一个将输入utterance $X$ 编码成向量的encoder。\n 2. 一个用来记录整个会话中对话状态的上下文RNN。它将utterance的向量作为输入，并在每一轮对话中输出一个用来指引生成回复的向量。\n 3. 一个用来生成response $Y$ 的decoder。\n 4. 一个用来在每轮对话执行事实匹配（facts matching）和实体发散（diffuses to similar entities）的知识搜索器（knowledge retriever）。\n\n**1. Encoder**\n\n为了捕捉不同层面的信息，作者通过两个独立的RNN来学习utterance的表示，得到两个隐藏状态序列：\n$H^C=(h_1^C, h_2^C, ..., h_{Nx}^C)$ 和 $H^K=(h_1^K, h_2^K, ..., h_{Nx}^K)$ 。\n$h_{Nx}^C$ 作为之后用来追踪对话状态的上下文RNN的输入；$h_{Nx}^K$ 应用在knowledge retriever中，用来将知识实体关系编码进输入的utterance。上图 $X_1$ 中\"director\"和\"Titanic\"就是knowledge elements。\n\n**2. Knowledge Retriver**\n\nKnowledge retriever从知识库中抽取固定数量的facts并指出它们的重要程度。\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f17.png\"></div>\n\n 1. **Facts Matching**\n\n给定一个input utterance $X$，通过字符串匹配（string matching）、实体链接（entity linking）或命名实体识别（named entity recognition）来从知识库和对话历史中抽取$N_f$个相关facts $F = \\{f_1,f_2,...,f_{N_f}\\}$。$F$ 中的每个triplet的embedding通过简单地将entities和relation的embedding取均值得到 $h_f = \\{h_{f_1},h_{f_2},...,h_{f_{N_f}}\\}$。\n\n之后计算每个fact和input utterance之间的相关系数（范围从0到1）：\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f18.png\"></div>\n\n对于多轮对话，出现在之前的utterance的实体也会继承并保存在 $F$ 中。作者通过加权平均来计算相关事实表示（*relevant fact representation*） $C^f$:\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f19.png\"></div>\n\n 1. **Entity Diffusion**\n\n作者计算了知识库中的所有实体（除了已经在之前的utterance出现过的实体）与相关fact的相似系数 $r^e$（范围从0到1）：\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f20.png\"></div>\n\n这里$e^k$是entity embedding。得分最高的$N_e$个实体被视作相似实体。相似实体表示（*similar entity representation*）$C^s$：\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f21.png\"></div>\n\n**3. Context RNN**\n\nContext RNN 记录了utterance (sentence) level对话状态。Context RNN考虑了utterance表示和知识表示。\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f22.png\"></div>\n\n**4. Decoder**\n\n$$C=[h_t^T, C^f, C^s]$$\n\n$$R=[r^f, r^e]$$\n\n 1. **Vanilla decoder** 仅通过 $C$, $R$ 产生response $Y$：\n   \n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f23.png\"></div>\n\n 2. **Probabilistic gated decoder** 使用了一个门控变量 $z_t$ 来表明位于第t个位置的词应该从普通词中产生还是从知识实体中产生：\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f24.png\"></div>\n\n其中 $p(z_t|s_t;\\theta)$ 通过逻辑回归来计算；$p(y_t|R,z_t=1;\\theta)$ 近似于知识项（knowledge items）的相关系数。\n\n考虑到在生成回复中，如果一个实体被反复使用，生成的回复的多样性将会减小。因此，当一个知识项出现在回复中时，其相应的相关系数应该减小，以避免一个item出现多次。为此，作者提出了两种范围追踪机制（coverage tracking mechanisms）：1） ***Mask coefficient tracker*** 直接将出现的item的系数减少到0来保证其不会在回复中再次出现；2） ***Coeffient attenuation tracker*** ：\n\n$$i_t=DNN(s_t, y_{t-1},R_0,R_{t-1}),$$\n\n$$R_t=i_t{\\cdot}R_{t-1}$$\n\n> **Experiment**\n\n**Detail**\n\nencoder使用512d Bi-LSTM；context RNN使用1024d LSTM；普通词汇、实体词、关系词共享同一个512d word embedding；使用Adam算法进行参数优化，梯度截断设为5.0。\n\n对比实验使用了3个baseline模型：\n\n- Seq2Seq：最基本的RNN encoder-decoder模型。\n\n- HERD：一个分级RNN encoder-decoder模型。\n\n- GenDS:一个生成式神经网络对话系统，具有从输入文本和相关知识库生成回复的功能[[Zhu et al., 2017]](https://arxiv.org/pdf/1709.04264.pdf)。\n\n尝试了3种NKD的变种模型：\n\n- NKD-ori：使用基础的decoder和一个mask coefficient tracker。\n\n- NKD-gated：加入了probabilistic gated decoder和一个mask coefficient tracker。\n\n- NKD-atte：使用基础的decoder和一个coefficient attenuation tracker。\n\n**Result**\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f25.png\" width = \"400\"></div>\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f26.png\" width = \"400\"></div>\n\n<br>\n\n# Mem2Seq: Effective Incorporating Knowledge Based into End-to-End Task-Orient Dialog Systems\n\n<https://arxiv.org/pdf/1804.08217.pdf>\n\n> **Motivation**\n\n近来，很多端到端对话系统都引入了基于注意力机制的copy-mechanism，将单词直接从输入文本copy到输出回复。使用这样的机制，使得即便是对话历史中出现了unknown的token，模型也能产生正确相关的实体词。\n\n然而，尽管copy-mechanism被证明是很成功的，但它同样面临这两个主要问题：1）出于对RNN不适合处理超长文本的限制的考虑，将外部KB信息引入RNN隐藏状态的有效性有待商榷。 2）处理长文本序列的时间开销巨大，尤其是引入注意力机制之后。\n\n另一方面，end-to-end memory networks[[Sukhbaatar et al., 2015]](https://arxiv.org/pdf/1503.08895.pdf)是一个在巨大外部记忆进行循环attention操作的模型。它将外部记忆编入几个embedding矩阵，利用query向量反复读取记忆。这种方法可以记忆外部KB信息，并且**快速地编码长对话历史**。另外这种multi-hop的机制从实验结果来看，其对于推理任务是极其重要的。但是，MemNN只能从预定义的候选池中选择回复而不是逐个生成单词；而且对记忆的query需要显式设计而不是通过模型自行学得，也缺少时下流行的copy机制。\n\n为了解决这些问题，论文提出了一个新的框架Mem2Seq，来通过端到端的方法学习生成面向特定任务的对话。总的来说，论文中的模型将现有的MemNN框架扩展为一个文本生成式架构，使用全局multi-hop注意力从历史对话和KB中直接copy单词生成回复。\n\n> **Model**\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f27.png\"></div>\n\n模型由两部分组成：MemNN encoder生成对话历史的向量表示；memory decoder从记忆中读取和copy来生成回复。\n\n对话历史定义为一个tokens序列：$X = \\{ x_1, \\dots, x_n, sentinel \\}$ , $sentinel$ 是一个标志字符；\n\nKB元组定义为 $B=\\{ b_1, \\dots, b_l \\}$；$U=[B;X]$;\n\n$Y=\\{ y_1, \\dots, y_m \\}$ 为预期输出序列;\n\n另设一个和预期输出对其的预期指针序列：$PTR=\\{ ptr_1, \\dots, ptr_m \\}$\n\n$$\nptr_i = \n\\begin{cases}\n\\max (z) \\,\\,\\,\\, if \\exists z \\, s.t.\\, y_i=u_z\\\\\nn+l+1 \\,\\,\\,\\, otherwise\n\\end{cases}\n$$\n\n**Memory Encoder**\n\nencoder只用来加载和编码dialogue history $X$。\n\nMemNN中的memories是一系列可训练的embedding矩阵 $C = \\{ C^1, \\dots , C^{K+1} \\}$，每个矩阵将tokens映射为vectors。\n\n$q^k$ 为query vector； $q^1$ 固定为零向量。\n\n模型将会循环K个hop：\n\n$$p_i^k = Softmax((q^k)^T C_i^k)$$\n\n$$C_i^k = C^k (x_i)$$\n\n$$o^k = \\sum_i p_i^k C_i^{k+1}$$\n\n$$q^{k+1} = q^k + o^k$$\n\n原版MemNN对memory有两个embedding matrices：input embedding matrix $A$ 和output embedding matrix $C$，分别表示存入memory时输入单词的embedding和读取memory时隐语义的embedding。Mem2Seq中实际上也有上述两个embedding matrices，并采用了MemNN中的adjacent weight tying策略，即 $A^{k+1}=C^k$。\n\nEncoder最后输出 $o^K$ 作为decoder的输入。（作者提供的源代码是将 $q^{K+1}$ 作为decoder输入）。\n\n**Memory Decoder**\n\nDecoder包含一个GRU和一个MemNN，MemNN加载dialogue history和KB的拼接 $U$。**Decoder的MemNN和encoder的MemNN不共享参数**。\n\n$$h_t=GRU(C^1(\\hat{y}_{t-1}),h_{t-1})$$\n\nEncoder的输出 $o^K$ 作为GRU的初始隐藏状态 $h_0$。\n\nt时刻，GRU的隐藏状态 $h_t$ 作为decoder MemNN的query vector，经过MemNN的K个hop，得到decoder memory output $o^K$。之后decoder会生成vocab和pointer两种分布：\n\n$$P_{vocab}(\\hat{y}_t)=Softmax(W_1[h_t;o^1])$$\n\n$$P_{ptr}(\\hat{y}_t)=p^K$$\n\n$o^1$ 为decoder MemNN的hop 1的memory output，$p^K$ 为decoder MemNN的hop K的attention score。\n\n**Memory Content**\n\nMemory整合了word-level的dialogue history和KB tuples。每个KB tuple的representation是subject, relation, object的embedding之和。如果pointer指向KB tuple，则输出为tuple的object。\n\n> **Experiment**\n\n**Detail**\n\n使用Adam optimizer，学习率在1e-3到1e-4之间。MemNN采用了hops K =  1, 3, 6三种来对比实验。Dropout rate在0.1到0.4之间，同时还将部分输入word mask成unknown token来模拟OOV情况。\n\n**Result**\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f28.png\"></div>\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f29.png\" width = \"400\"></div>\n\n<div align=center><img src=\"/img/knowledge-based-text-generation/knowledge-based-text-generation-f30.png\" width = \"400\"></div>\n","tags":["deep learning","NLP","NLG","dialogue system","common sense","papers"],"categories":["NLP"]},{"title":"markdown数学表达式","url":"/2019/02/27/markdown-math-expression/","content":"平方根式：\n```\n$$r=\\sqrt{x^2+y^2}$$ \n```\n$$r=\\sqrt{x^2+y^2}$$\n\n分数式：\n```\n$$P=\\frac{TP}{TP+FP}$$\n```\n$$P=\\frac{TP}{TP+FP}$$\n\n求和与连乘：\n```\n$$\\sum_{i=0}^n\\prod_{i=0}^n$$\n```\n$$\\sum_{i=0}^n\\prod_{i=0}^n$$\n\n积分：\n```\n$$\\int_0^{\\pi}f(x)\\,dx$$\n```\n$$\\int_0^{\\pi}f(x)\\,dx$$\n\n极限：\n```\n$$\\lim_{x \\to + \\infty}\\frac{1}{x}$$\n```\n$$\\lim_{x \\to + \\infty}\\frac{1}{x}$$\n\n偏导：\n```\n$$\\frac{\\partial^2 u}{\\partial x \\partial y}$$\n```\n$$\\frac{\\partial^2 u}{\\partial x \\partial y}$$\n\n矩阵：\n```\n$$\nA=\\left[\n\\begin{matrix}\n18&21&6\\\\\n21&25&7\\\\\n6&7&2\n\\end{matrix}\n\\right]\n$$\n```\n$$\nA=\\left[\n\\begin{matrix}\n18&21&6\\\\\n21&25&7\\\\\n6&7&2\n\\end{matrix}\n\\right]\n$$\n\n分段函数式：\n```\n$$\ny_i{\\cdot}g(x_i)=\n\\begin{cases}\n\\ge 1, \\,\\,\\,\\, \\{x_i \\mid \\alpha_i = 0\\}\\\\\n= 1, \\,\\,\\,\\, \\{x_i \\mid 0 \\lt \\alpha_i \\lt C\\}\\\\\n\\le 1, \\,\\,\\,\\, \\{x_i \\mid \\alpha_i = C\\}\n\\end{cases}\n$$\n```\n$$\ny_i{\\cdot}g(x_i)=\n\\begin{cases}\n\\ge 1, \\,\\,\\,\\, \\{x_i \\mid \\alpha_i = 0\\}\\\\\n= 1, \\,\\,\\,\\, \\{x_i \\mid 0 \\lt \\alpha_i \\lt C\\}\\\\\n\\le 1, \\,\\,\\,\\, \\{x_i \\mid \\alpha_i = C\\}\n\\end{cases}\n$$\n\n数学符号表：\n\n<div align=center> <img src=\"/img/markdown-math-expression/markdown-math-expression-f1.png\" alt=\"\">\n\n<div align=center> <img src=\"/img/markdown-math-expression/markdown-math-expression-f2.png\" alt=\"\">\n\n<div align=center> <img src=\"/img/markdown-math-expression/markdown-math-expression-f3.png\" alt=\"\">\n\n<div align=center> <img src=\"/img/markdown-math-expression/markdown-math-expression-f4.png\" alt=\"\">\n\n<div align=center> <img src=\"/img/markdown-math-expression/markdown-math-expression-f5.png\" alt=\"\">\n\n<div align=center> <img src=\"/img/markdown-math-expression/markdown-math-expression-f6.png\" alt=\"\">\n\n<div align=center> <img src=\"/img/markdown-math-expression/markdown-math-expression-f7.png\" alt=\"\">\n\n<div align=center> <img src=\"/img/markdown-math-expression/markdown-math-expression-f8.png\" alt=\"\">\n\n<div align=center> <img src=\"/img/markdown-math-expression/markdown-math-expression-f9.png\" alt=\"\">\n\n<div align=center> <img src=\"/img/markdown-math-expression/markdown-math-expression-f10.png\" alt=\"\">\n\n<br>\n<br>\n\n参考来源：<http://www.mohu.org/info/lshort-cn.pdf>\n","tags":["etc"],"categories":["Uncategorized"]}]