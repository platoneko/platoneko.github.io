<!DOCTYPE html>
<html lang="en">







<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<link rel="preconnect" href="//www.googletagmanager.com">
	<link rel="preconnect" href="//zz.bdstatic.com">
	<link rel="preconnect" href="//sp0.baidu.com">
	<link rel="preconnect" href="//www.google-analytics.com">
	<link rel="preconnect" href="//cdn1.lncld.net">
	<link rel="preconnect" href="//unpkg.com">
	<link rel="preconnect" href="//app-router.leancloud.cn">
	<link rel="preconnect" href="//9qpuwspm.api.lncld.net">
	<link rel="preconnect" href="//gravatar.loli.net">

	<title>变分自编码器漫谈 | 𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸’𝓼 𝓝𝓸𝓽𝓮</title>

	<meta name="HandheldFriendly" content="True">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
	<meta name="generator" content="hexo">
	<meta name="author" content="𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸">
	<meta name="description" content="">

	
	<meta name="keywords" content="">
	

	
	<link rel="shortcut icon" href="/img/icon.jpg">
	<link rel="apple-touch-icon" href="/img/icon.jpg">
	

	
	<meta name="theme-color" content="#3c484e">
	<meta name="msapplication-TileColor" content="#3c484e">
	

	

	

	
	<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
	</script>
	<script type="text/x-mathjax-config">
		MathJax.Hub.Queue(function() {
			var all = MathJax.Hub.getAllJax(), i;
			for(i=0; i < all.length; i += 1) {
				all[i].SourceElement().parentNode.className += ' has-jax';
			}
		});
	</script>
	<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
        

	<meta property="og:site_name" content="𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸’𝓼 𝓝𝓸𝓽𝓮">
	<meta property="og:type" content="article">
	<meta property="og:title" content="变分自编码器漫谈 | 𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸’𝓼 𝓝𝓸𝓽𝓮">
	<meta property="og:description" content="">
	<meta property="og:url" content="http://yoursite.com/2019/04/05/vae/">

	
	<meta property="article:published_time" content="2019-04-05T14:04:00+08:00"> 
	<meta property="article:author" content="𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸">
	<meta property="article:published_first" content="𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸’𝓼 𝓝𝓸𝓽𝓮, /2019/04/05/vae/">
	

	
	
	<link rel="stylesheet" href="/css/allinonecss.min.css">

	
	
</head>

<body class="post-template">
	<div class="site-wrapper">
		




<header class="site-header post-site-header outer">
    <div class="inner">
        
<nav class="site-nav"> 
    <div class="site-nav-left">
        <ul class="nav">
            <li>
                
                
                <a class="site-nav-logo" href="/" title="𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸’𝓼 𝓝𝓸𝓽𝓮">
                    <img src="/img/icon.jpg" alt="𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸’𝓼 𝓝𝓸𝓽𝓮">
                </a>
                
                
            </li>
            
            
            <li>
                <a href="/" title="HOME">HOME</a>
            </li>
            
            <li>
                <a href="/about" title="ABOUT">ABOUT</a>
            </li>
            
            <li>
                <a href="/archives" title="ARCHIVES">ARCHIVES</a>
            </li>
            
            
        </ul> 
    </div>
    
    <div class="search-button-area">
        <a href="#search" class="search-button">Search ...</a>
    </div>
     
    <div class="site-nav-right">
        
        <a href="#search" class="search-button">Search ...</a>
         
        
<div class="social-links">
    
    
    <a class="social-link" title="github" href="https://github.com/platoneko" target="_blank" rel="noopener">
        <svg viewbox="0 0 1049 1024" xmlns="http://www.w3.org/2000/svg"><path d="M524.979332 0C234.676191 0 0 234.676191 0 524.979332c0 232.068678 150.366597 428.501342 358.967656 498.035028 26.075132 5.215026 35.636014-11.299224 35.636014-25.205961 0-12.168395-0.869171-53.888607-0.869171-97.347161-146.020741 31.290159-176.441729-62.580318-176.441729-62.580318-23.467619-60.841976-58.234462-76.487055-58.234463-76.487055-47.804409-32.15933 3.476684-32.15933 3.476685-32.15933 53.019436 3.476684 80.83291 53.888607 80.83291 53.888607 46.935238 79.963739 122.553122 57.365291 152.97411 43.458554 4.345855-33.897672 18.252593-57.365291 33.028501-70.402857-116.468925-12.168395-239.022047-57.365291-239.022047-259.012982 0-57.365291 20.860106-104.300529 53.888607-140.805715-5.215026-13.037566-23.467619-66.926173 5.215027-139.067372 0 0 44.327725-13.906737 144.282399 53.888607 41.720212-11.299224 86.917108-17.383422 131.244833-17.383422s89.524621 6.084198 131.244833 17.383422C756.178839 203.386032 800.506564 217.29277 800.506564 217.29277c28.682646 72.1412 10.430053 126.029806 5.215026 139.067372 33.897672 36.505185 53.888607 83.440424 53.888607 140.805715 0 201.64769-122.553122 245.975415-239.891218 259.012982 19.121764 16.514251 35.636014 47.804409 35.636015 97.347161 0 70.402857-0.869171 126.898978-0.869172 144.282399 0 13.906737 9.560882 30.420988 35.636015 25.205961 208.601059-69.533686 358.967656-265.96635 358.967655-498.035028C1049.958663 234.676191 814.413301 0 524.979332 0z"/></svg>
    </a>
    
    
    
    
    
    
</div>
    </div>
</nav>
    </div>
</header>


<div id="site-main" class="site-main outer" role="main">
    <div class="inner">
        <header class="post-full-header">
            <div class="post-full-meta">
                <time class="post-full-meta-date" datetime="2019-04-05T06:51:57.000Z">
                    2019-04-5
                </time>
                
                <span class="date-divider">/</span>
                
                <a href="/categories/deep-learning/">deep learning</a>&nbsp;&nbsp;
                
                
            </div>
            <h1 class="post-full-title">变分自编码器漫谈</h1>
        </header>
        <div class="post-full no-image">
            
            <div class="post-full-content">
                <article id="photoswipe" class="markdown-body">
                    <h1 id="从PCA说起"><a href="#从PCA说起" class="headerlink" title="从PCA说起"></a>从PCA说起</h1><p>主成分分析，或者称为PCA，是一种广泛使用的用来降低维度、抽取特征的技术。降维可以分为线性降维和非线性降维，PCA就是一种线性降维的方法，通过对原始特征做线性变换（由于加了偏置项 $b$ 其实是仿射变换），将数据投影到一个低维空间。那么应该做怎样的线性变换，投影到一个怎样的低维空间呢？PCA希望投影后的样本具有 <strong>1.最近重构性：用样本经过投影变换后得到的新坐标来重构原始坐标，应该有最小的平方误差；2.最大可分性：投影后的样本点在新空间中应该尽可能分开，即使投影后的样本点的方差最大化。</strong> 有趣的是，基于最近重构性和最大可分性推导，能分别得到同一组线性变换参数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min_W -tr(W^T X X^T W)\\
&s.t. \, W^T W = I
\end{aligned}</script><p>$W$ 就是我们要求的线性变换，对输入样本 $x$ 进行降维即求 $z = W^T x$，$z$ 为中间隐变量，从 $z$ 重构 $x$ 即求 $\hat{x} = W z = W W^T x$ 。</p>
<a id="more"></a>
<h1 id="非生成式模型-gt-生成式模型"><a href="#非生成式模型-gt-生成式模型" class="headerlink" title="非生成式模型 -&gt; 生成式模型"></a>非生成式模型 -&gt; 生成式模型</h1><p>前面介绍的PCA的形式是基于将数据线性投影到低维空间内。我们可以很容易地通过PCA计算输入数据 $x$ 在低维空间的表示 $z$ ，在实际应用中我们也更关注的是PCA的降维过程（编码），而不是重构过程（解码）。毕竟PCA的解码过程只能从已有的样本 $x$ 的隐变量 $z$ 去重构回 $x$ 自己，这根本就不具备生成功能，而我们现在想要一个能够生成新样本的模型。</p>
<p>那么PCA能用来生成新样本吗？答案是可以的，PCA具备这样的潜能，但是需要从另一种概率角度来推导和理解PCA，才能把PCA纳入生成式模型。这种生成式的PCA被称作概率PCA。</p>
<p>原始的PCA直接利用数据集 $X$ 的结构信息建模一个最优化问题，然后求解这个问题得到每个 $x_i$ 对应的 $z_i$ 。而概率PCA认为，数据集 $X$ 是对随机变量 $x$ 的若干次采样得到的，$x$ 依赖于随机变量 $z$ 。</p>
<p>概率PCA定义隐变量 $z$ 的先验概率分布为标准高斯分布：</p>
<script type="math/tex; mode=display">
z \sim p(z) = \mathcal{N} (z;0, I)</script><p>接着定义观测变量 $x$ 的高斯条件概率分布 $p(x \mid z)$ 。因为PCA中 $x$ 与 $z$ 的线性关系 $x = Wz + \mu$，观测变量 $x$ 的条件概率分布还是高斯分布：</p>
<script type="math/tex; mode=display">
p(x \mid z) = \mathcal{N} (x; Wz + \mu, \sigma^2 I)</script><p>有了上面两式，我们就可以首先为隐变量从标准高斯分布采样一个值 $z$，然后以这个隐变量为条件，对观测变量 $x$ 进行采样来生成新样本：</p>
<script type="math/tex; mode=display">
x = Wz + \mu + \epsilon \, ,\epsilon \sim \mathcal{N} (0, \sigma^2 I)</script><p>那么现在的问题就是，如何确定参数 $W$, $\mu$, $\sigma$ 。我们可以使用极大似然的方式来确定参数，为了写出似然函数的表达式，我们需要观测变量的边缘概率分布的表达式：</p>
<script type="math/tex; mode=display">
p(x) = \int p(x \mid z) p(z)dz</script><p>由于这对应于一个线性高斯模型，因此边缘分布还是高斯分布：</p>
<script type="math/tex; mode=display">
\mathbb{E}[x] = \mathbb{E}[Wz + \mu + \epsilon] = \mu</script><script type="math/tex; mode=display">
\begin{aligned}
cov[x] &= \mathbb{E}[(Wz + \epsilon) (Wz + \epsilon)^T]\\
&= \mathbb{E}[Wz z^T W^T] + \mathbb{E}[\epsilon \epsilon^T]\\
&= WW^T + \sigma^2 I
\end{aligned}</script><script type="math/tex; mode=display">
p(x) = \mathcal{N}(x;\mu,WW^T + \sigma^2 I)</script><p>之后就可以通过最大化对数似然来确定模型的参数了。</p>
<h1 id="线性模型-gt-非线性模型"><a href="#线性模型-gt-非线性模型" class="headerlink" title="线性模型 -&gt; 非线性模型"></a>线性模型 -&gt; 非线性模型</h1><p>前面的PCA以及概率PCA都是一种线性模型，它假设隐变量 $z$ 是观测变量 $x$ 的线性投影，这样的假设必然有它的局限性。而自编码器AutoEncoders则是一种非线性的降维模型。AutoEncoders的非线性在于，它在编码（降维）和解码（重构）的过程，都是用多层感知机，利用堆叠非线性激活函数来近似拟合任何函数。AutoEncoders是一种特殊的神经网络，他的输入和输出相同，训练目标是最小化重构的平方误差。可以说自编码器就是PCA的非线性版本，因此原始的自编码器也不具备生成式模型的功能。</p>
<p>从之前的PCA到概率PCA，我们发现一个降维模型要升级到生成式模型，必须要有一个隐变量 $z$ 的先验分布，生成时要先对 $z$ 进行采样才能对新的样本 $x$ 进行重构。那我们再次将贝叶斯学派的概率图模型引入自编码器，于是就有了具备生成新样本能力的变分自编码器VAE了。</p>
<p>将概率模型引入自编码器，和前面的概率PCA一样，我们想要最大化边缘概率分布 $p_\theta (x) = \int p_\theta (z) p_\theta (x \mid z)dz$ 。但是和前面的概率PCA不同，我们这里的 $p(x \mid z)$ 是通过神经网络拟合的复杂非线性函数，这就使得上面的积分式不再可积，也无法通过微分方法优化参数。而且后验分布 $p_\theta (z \mid x) = \frac{p_\theta (x \mid z) p_\theta (z)}{p_\theta (x)}$ 因为分母不可积，也无法使用EM算法来优化参数。于是VAE的作者引入了变分推断：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\log p_\theta(x) =& \log p_\theta(x, z) - \log p_\theta(z \mid x)\\
=& \log \frac{p_\theta(x, z)}{q_\phi (z \mid x)} - \log \frac{p_\theta(z \mid x)}{q_\phi  (z \mid x)}\\
=& \log p_\theta(x, z) - \log q_\phi (z \mid x) + \log \frac{q_\phi  (z \mid x)}{p_\theta(z \mid x)}\\
\end{aligned}</script><p>等式两边同时对 $q_\phi (z \mid x)$ 求期望：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int q_\phi (z \mid x) \log p_\theta(x)dz =& \int q_\phi (z \mid x) \log p_\theta(x, z)dz - \int q_\phi (z \mid x) \log q_\phi (z \mid x)dz\\ &+ \int q_\phi (z \mid x) \log \frac{q_\phi  (z \mid x)}{p_\theta(z \mid x)}dz\\
\log p_\theta(x) \int q_\phi (z \mid x) dz =& \mathbb{E}_{ {q_\phi} (z \mid x)}[\log p_\theta (x,z) - \log q_\phi (z \mid x)] + KL(q_\phi (z \mid x) \| p_\theta (z \mid x))\\
\log p_\theta (x) =& KL(q_\phi (z \mid x) \| p_\theta (z \mid x)) + \mathcal{L}(\theta , \phi;x)
\end{aligned}</script><p>由于KL散度的非负性，我们可以通过优化上式第二项变分下界来优化参数:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\log p_\theta (x) \ge \mathcal{L}(\theta , \phi;x) &= \mathbb{E}_{ {q_\phi} (z \mid x)}[-\log q_\phi (z \mid x) + \log p_\theta (x,z)]\\
&= -KL(q_\phi (z \mid x) \| p_\theta (z)) + \mathbb{E}_{ {q_\phi} (z \mid x)} [\log p_\theta (x\mid z)]
\end{aligned}</script><p>可见VAE并没有使用 $p(z)$ 是高斯分布的假设，而是假设后验分布 $p(z \mid x)$ 是高斯分布。具体来说，在训练过程中，给定真实一个样本 $x$ ，我们假设存在一个专属于 $x$ 的分布 $p(z \mid x)$ ，并假设这个分布是高斯分布。直观来看，因为我们后面要训练一个生成器 $X = g(Z)$，希望能够把从“专属”分布 $p(z \mid x_k)$ 中采样出一个 $z_k$ 还原为 $x_k$。如果训练中，直接假设 $p(z)$ 是高斯分布并从中采样一个 $z$ ，我们就无法确定这个 $z$ 对应于哪个真实的 $x$。</p>
<p>那如何找到后验分布 $p(z \mid x)$ 呢？神经网络时代的哲学：遇事不决，神经网络！我们直接通过两个MLP把 $p(z \mid x)$ 的均值和方差拟合出来，拟合成一个近似结果 $q_\phi (z \mid x)$ 。这个步骤其实也没那么粗暴，只不过是深度学习的基本操作，这不就是我们常用的自编码器的编码部分吗？</p>
<p>有了这些，我们现在可以来关注一下目标函数的优化：</p>
<script type="math/tex; mode=display">
\mathcal{L}(\theta , \phi;x) = -KL(q_\phi (z \mid x) \| p_\theta (z)) + \mathbb{E}_{ {q_\phi} (z \mid x)} [\log p_\theta (x\mid z)]</script><p>第一项是个KL散度。因为这是个生成式模型，我们用它生成的时候要先从一个先验分布 $p(z)$ 中采样出一个 $z$ 再输入解码器中来还原出 $x$ 来。这里干脆就定义这个先验分布为标准高斯分布。那么KL散度的计算结果就是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
KL(q_\phi  (z \mid x) \| p_\theta (z)) &= KL(\mathcal{N}(\mu, \sigma^2 I) \| \mathcal{N}(0, I))\\
&= \frac{1}{2} \sum_{i=1}^d (\mu_{(i)}^2 + \sigma_{(i)}^2 - \log \sigma_{(i)}^2 - 1)
\end{aligned}</script><p>由于考虑的是各分量独立多元高斯分布，因此只需要推导一元高斯分布的情形即可：</p>
<script type="math/tex; mode=display">
\begin{aligned}&KL(N(\mu,\sigma^2)\Vert N(0,1))\\
=&\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} (\log \frac{e^{-(x-\mu)^2/2\sigma^2}/\sqrt{2\pi\sigma^2}}{e^{-x^2/2}/\sqrt{2\pi}})dx\\
=&\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \log \{\frac{1}{\sqrt{\sigma^2}}\exp\{\frac{1}{2}[x^2-(x-\mu)^2/\sigma^2]\} \}dx\\
=&\frac{1}{2}\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} [-\log \sigma^2+x^2-(x-\mu)^2/\sigma^2] dx\end{aligned}</script><p>第一项是 $-log \sigma^2$ 乘以概率密度的积分（相当于乘1），第二项是高斯分布二阶矩 $\mu^2 + \sigma^2$，第三项是”-方差/方差”结果就是-1。</p>
<p>接下来就是目标函数的第二项，第二项对应于解码重构的过程。$\mathbb{E}_{ {q_\phi} (z \mid x)} [\log p_\theta (x\mid z)]$ 就等价于 $\frac{1}{n} \sum^N_i \log p_\theta (x\mid z_i) \,\, , z_i \sim {q_\phi} (z \mid x)$，我们只需从编码过程计算出的后验高斯分布中采样出一个 $z$ ，然后去计算 $\log p_\theta (x\mid z)$ 即可。但是这里看上去好像有点问题：采样一个够吗？中山大学苏神：“事实上我们会运行多个epoch，每次的隐变量都是随机生成的，因此当epoch数足够多时，事实上是可以保证采样的充分性的。我也实验过采样多个的情形，感觉生成的样本并没有明显变化。”那就这样吧。</p>
<p>那生成模型中的条件分布 $p_\theta (x\mid z)$ 是啥呢？论文中给出了两种分布：伯努利分布（用于二值的 $x$）和高斯分布（用于连续实数的 $x$）。</p>
<p>伯努利分布模型：</p>
<script type="math/tex; mode=display">
q(x|z)=\prod_{k=1}^D (\rho_{(k)}(z))^{x_{(k)}} (1 - \rho_{(k)}(z))^{1 - x_{(k)}}</script><script type="math/tex; mode=display">
-\ln q(x|z) = \sum_{k=1}^D [- x_{(k)} \ln \rho_{(k)}(z) - (1-x_{(k)}) \ln (1 -\rho_{(k)}(z))]</script><p>高斯分布模型：</p>
<script type="math/tex; mode=display">
q(x|z)=\frac{1}{\prod\limits_{k=1}^D \sqrt{2\pi  \sigma_{(k)}^2(z)}}\exp(-\frac{1}{2}\Vert\frac{x-\mu(z)}{\sigma(z)}\Vert^2)</script><script type="math/tex; mode=display">
-\ln q(x|z) = \frac{1}{2}\Vert\frac{x-\mu(z)}{\sigma(z)}\Vert^2 + \frac{D}{2}\ln 2\pi + \frac{1}{2}\sum_{k=1}^D \ln \sigma_{(k)}^2(z)</script><p>但是我们重构 $x$ 时并不会真的从这个高斯分布中采样，而是直接以高斯分布概率密度最大点（即均值）作为重构结果，因此我们用不上方差 $\sigma^2$。因为如果是采样出 $x$，那模型就无法通过微分方法（梯度下降）优化参数了。这时候：</p>
<script type="math/tex; mode=display">
-\ln q(x|z) \sim \frac{1}{2\sigma^2}\Vert x-\mu(z)\Vert^2</script><p>可以发现，其形式就是一个MSE损失。</p>
<p>可是现在还有一个问题：如果 $z$ 是通过预测后验分布 $q_\phi (z \mid x)$ 采样取得，采样过程是不可微的，那我们要怎样才能优化编码器部分的参数呢？这里，VAE使用了一个重参数技巧：从 $\mathcal{N}(\mu, \sigma^2)$ 中采样一个 $z$ 可以等价于从 $\mathcal{N}(0,I)$ 中采样一个 $\varepsilon$ 使 $z = \mu + \varepsilon \times \sigma$ 。这样一来，“采样”这个操作就不用参与梯度下降了，改为采样的结果参与，使得整个模型可训练了。整个VAE框架的流程就完成了。</p>
<h1 id="如果直面联合分布？"><a href="#如果直面联合分布？" class="headerlink" title="如果直面联合分布？"></a>如果直面联合分布？</h1><p>考虑从另一角度推导VAE的目标函数：干脆直接来对 $p(x,z)$ 做近似。定义 $p(x,z)=\tilde{p}(x)p(z|x)$，设想用联合分布 $q(x,z)$ 来近似 $p(x,z)$：</p>
<script type="math/tex; mode=display">
KL(p(x,z)\Vert q(x,z)) = \iint p(x,z)\ln \frac{p(x,z)}{q(x,z)} dzdx</script><script type="math/tex; mode=display">
\begin{aligned}KL(p(x,z)\Vert q(x,z)) =& \int \tilde{p}(x) [\int p(z|x)\ln \frac{\tilde{p}(x)p(z|x)}{q(x,z)} dz]dx\\ 
=& \mathbb{E}_{x\sim \tilde{p}(x)} [\int p(z|x)\ln \frac{\tilde{p}(x)p(z|x)}{q(x,z)} dz] 
\end{aligned}</script><script type="math/tex; mode=display">
\ln \frac{\tilde{p}(x)p(z|x)}{q(x,z)}=\ln \tilde{p}(x) + \ln \frac{p(z|x)}{q(x,z)}</script><script type="math/tex; mode=display">
\begin{aligned}\mathbb{E}_{x\sim \tilde{p}(x)} [\int p(z|x)\ln \tilde{p}(x)dz] =& \mathbb{E}_{x\sim \tilde{p}(x)} [\ln \tilde{p}(x)\int p(z|x)dz]\\ 
=&\mathbb{E}_{x\sim \tilde{p}(x)} [\ln \tilde{p}(x)] 
\end{aligned}</script><p>注意到 $\mathbb{E}_{x\sim \tilde{p}(x)} [\ln \tilde{p}(x)]$ 是一个常数项，因此目标函数可以简化为：</p>
<script type="math/tex; mode=display">
\begin{aligned}\mathcal{L} =& \mathbb{E}_{x\sim \tilde{p}(x)} [\int p(z|x)\ln \frac{p(z|x)}{q(x,z)} dz]\\
=& \mathbb{E}_{x\sim \tilde{p}(x)} [\int p(z|x)\ln \frac{p(z|x)}{q(x|z)q(z)} dz]\\ 
=&\mathbb{E}_{x\sim \tilde{p}(x)} [-\int p(z|x)\ln q(x|z)dz+\int p(z|x)\ln \frac{p(z|x)}{q(z)}dz]\end{aligned}</script><p>最终形式：</p>
<script type="math/tex; mode=display">
\begin{aligned}\mathcal{L} = &\mathbb{E}_{x\sim \tilde{p}(x)} [\mathbb{E}_{z\sim p(z|x)}[-\ln q(x|z)]+\mathbb{E}_{z\sim p(z|x)}[\ln \frac{p(z|x)}{q(z)}]]\\
= &\mathbb{E}_{x\sim \tilde{p}(x)} [\mathbb{E}_{z\sim p(z|x)}[-\ln q(x|z)]+KL(p(z|x)\Vert q(z))]
\end{aligned}</script><p>这正是VAE的损失函数，我们直面联合分布，中间没有对后验分布进行分析，居然直接就得到了这个结果。</p>
<h1 id="VAE损失函数的直观理解"><a href="#VAE损失函数的直观理解" class="headerlink" title="VAE损失函数的直观理解"></a>VAE损失函数的直观理解</h1><p>损失函数中，最为直观的就是重构损失，它要求重构后的 $X$ 和原始的 $X$ 尽可能接近。如果仅仅只有重构损失，那么VAE就算前面做了再多努力，又是拟合均值方差又是后验采样，其最终还是会退化成一个普通的自编码器，失去了生成的功能。虽然 $z$ 是通过采样得到的，采样就会有噪声，显然噪声会增加重构的难度，不过好在这个噪声强度（也就是方差）通过一个神经网络算出来的，所以最终模型为了重构得更好，肯定会想尽办法让方差为0。而方差为0的话，也就没有随机性了，所以不管怎么采样其实都只是得到确定的结果（也就是均值）。这时，KL散度的作用就显现了，它要求所有的 $p(X \mid Z)$ 都要向标准高斯分布靠拢，这样就防止了噪声为0，同时保证了生成过程能放心地从标准高斯分布中采样一个 $z$ 来产生新样本：</p>
<script type="math/tex; mode=display">
p(Z)=\sum_X p(Z|X)p(X)=\sum_X \mathcal{N}(0,I)p(X)=\mathcal{N}(0,I) \sum_X p(X) = \mathcal{N}(0,I)</script><p>而且，KL损失和重构损失还存在对抗关系。直觉上来想，当decoder还没有训练好时（重构误差远大于KL loss），就会适当降低噪声（KL loss增加），使得拟合起来容易一些（重构误差开始下降）；反之，如果decoder训练得还不错时（重构误差小于KL loss），这时候噪声就会增加（KL loss减少），使得拟合更加困难了（重构误差又开始增加），这时候decoder就要想办法提高它的生成能力了。（浑然天成！）</p>

                </article>
                <ul class="tags-postTags">
                    
                    <li>
                        <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
                    </li>
                    
                    <li>
                        <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </div>

    
    <nav id="gobottom" class="pagination">
        
        <span class="prev-next-post">·</span>
        
        <a class="next-post" title="关于交叉熵的一些理解" href="/2019/03/21/cross-entropy/">
            关于交叉熵的一些理解 →
        </a>
        
    </nav>

    
    <div class="inner">
        <div id="comment"></div>
    </div>
    
</div>

<div class="toc-bar">
    <div class="toc-btn-bar">
        <a href="#site-main" class="toc-btn">
            <svg viewbox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M793.024 710.272a32 32 0 1 0 45.952-44.544l-310.304-320a32 32 0 0 0-46.4 0.48l-297.696 320a32 32 0 0 0 46.848 43.584l274.752-295.328 286.848 295.808z"/></svg>
        </a>
        <div class="toc-btn toc-switch">
            <svg class="toc-open" viewbox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M779.776 480h-387.2a32 32 0 0 0 0 64h387.2a32 32 0 0 0 0-64M779.776 672h-387.2a32 32 0 0 0 0 64h387.2a32 32 0 0 0 0-64M256 288a32 32 0 1 0 0 64 32 32 0 0 0 0-64M392.576 352h387.2a32 32 0 0 0 0-64h-387.2a32 32 0 0 0 0 64M256 480a32 32 0 1 0 0 64 32 32 0 0 0 0-64M256 672a32 32 0 1 0 0 64 32 32 0 0 0 0-64"/></svg>
            <svg class="toc-close hide" viewbox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M512 960c-247.039484 0-448-200.960516-448-448S264.960516 64 512 64 960 264.960516 960 512 759.039484 960 512 960zM512 128.287273c-211.584464 0-383.712727 172.128262-383.712727 383.712727 0 211.551781 172.128262 383.712727 383.712727 383.712727 211.551781 0 383.712727-172.159226 383.712727-383.712727C895.712727 300.415536 723.551781 128.287273 512 128.287273z"/><path d="M557.05545 513.376159l138.367639-136.864185c12.576374-12.416396 12.672705-32.671738 0.25631-45.248112s-32.704421-12.672705-45.248112-0.25631l-138.560301 137.024163-136.447897-136.864185c-12.512727-12.512727-32.735385-12.576374-45.248112-0.063647-12.512727 12.480043-12.54369 32.735385-0.063647 45.248112l136.255235 136.671523-137.376804 135.904314c-12.576374 12.447359-12.672705 32.671738-0.25631 45.248112 6.271845 6.335493 14.496116 9.504099 22.751351 9.504099 8.12794 0 16.25588-3.103239 22.496761-9.247789l137.567746-136.064292 138.687596 139.136568c6.240882 6.271845 14.432469 9.407768 22.65674 9.407768 8.191587 0 16.352211-3.135923 22.591372-9.34412 12.512727-12.480043 12.54369-32.704421 0.063647-45.248112L557.05545 513.376159z"/></svg>
        </div>
        <a href="#gobottom" class="toc-btn">
            <svg viewbox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M231.424 346.208a32 32 0 0 0-46.848 43.584l297.696 320a32 32 0 0 0 46.4 0.48l310.304-320a32 32 0 1 0-45.952-44.544l-286.848 295.808-274.752-295.36z"/></svg>
        </a>
    </div>
    <div class="toc-main">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#从PCA说起"><span class="toc-text">从PCA说起</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#非生成式模型-gt-生成式模型"><span class="toc-text">非生成式模型 -&gt; 生成式模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#线性模型-gt-非线性模型"><span class="toc-text">线性模型 -&gt; 非线性模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#如果直面联合分布？"><span class="toc-text">如果直面联合分布？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#VAE损失函数的直观理解"><span class="toc-text">VAE损失函数的直观理解</span></a></li></ol>
    </div>
</div>



<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>




	</div>
	


<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
            

<article class="read-next-card" style="background-image: url(/img/wallpaper.jpg)">
  <header class="read-next-card-header">
    <small class="read-next-card-header-sitetitle">&mdash; 𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸’𝓼 𝓝𝓸𝓽𝓮 &mdash;</small>
    <h3 class="read-next-card-header-title">Recent Posts</h3>
  </header>
  <div class="read-next-divider">
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
      <path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/>
    </svg>
  </div>
  <div class="read-next-card-content">
    <ul>
      
      
      
      <li>
        <a href="/2019/04/05/vae/">变分自编码器漫谈</a>
      </li>
      
      
      
      <li>
        <a href="/2019/03/21/cross-entropy/">关于交叉熵的一些理解</a>
      </li>
      
      
      
      <li>
        <a href="/2019/02/28/knowledge-based-text-generation/">基于知识的对话系统</a>
      </li>
      
      
      
      
    </ul>
  </div>
  <footer class="read-next-card-footer">
    <a href="/archives">  MORE  → </a>
  </footer>
</article>

            
            
            

<article class="read-next-card" style="background-image: url(/img/wallpaper.jpg)">
    <header class="read-next-card-header tagcloud-card">
        <h3 class="read-next-card-header-title">Categories</h3>
    </header>
    <div class="read-next-card-content">
        <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Uncategorized/">Uncategorized</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a></li></ul>
    </div>
</article>


            
            
            

<article class="read-next-card" style="background-image: url(/img/wallpaper.jpg)">
	<header class="read-next-card-header tagcloud-card">
		<h3 class="read-next-card-header-title">Tag Cloud</h3>
	</header>
	<div class="read-next-card-content-ext">
		<a href="/tags/NLG/" style="font-size: 14px;">NLG</a> <a href="/tags/NLP/" style="font-size: 14px;">NLP</a> <a href="/tags/common-sense/" style="font-size: 14px;">common sense</a> <a href="/tags/deep-learning/" style="font-size: 24px;">deep learning</a> <a href="/tags/dialogue-system/" style="font-size: 14px;">dialogue system</a> <a href="/tags/etc/" style="font-size: 14px;">etc</a> <a href="/tags/machine-learning/" style="font-size: 19px;">machine learning</a> <a href="/tags/papers/" style="font-size: 14px;">papers</a>
	</div>
</article>

            
        </div>
    </div>
</aside>

	




<div id="search" class="search-overlay">
    <div class="search-form">
        
        <div class="search-overlay-logo">
        	<img src="/img/icon.jpg" alt="𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸’𝓼 𝓝𝓸𝓽𝓮">
        </div>
        
        <input id="local-search-input" class="search-input" type="text" name="search" placeholder="Search ...">
        <a class="search-overlay-close" href="#"></a>
    </div>
    <div id="local-search-result"></div>
</div>

<footer class="site-footer outer">
	<div class="site-footer-content inner">
		<div class="copyright">
			<a href="/" title="𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸’𝓼 𝓝𝓸𝓽𝓮">𝓟𝓵𝓪𝓽𝓸𝓷𝓮𝓴𝓸’𝓼 𝓝𝓸𝓽𝓮 &copy; 2019</a>
			
				
			        <span hidden="true" id="/2019/04/05/vae/" class="leancloud-visitors" data-flag-title="变分自编码器漫谈">
			            <span>阅读量 </span>
			            <span class="leancloud-visitors-count">0</span>
			        </span>
	    		
    		
		</div>
		<nav class="site-footer-nav">
			
			<a href="https://hexo.io" title="Hexo" target="_blank" rel="noopener">Hexo</a>
			<a href="https://github.com/xzhih/hexo-theme-casper" title="Casper" target="_blank" rel="noopener">Casper</a>
		</nav>
	</div>
</footer>
	


<script>
    if(window.navigator && navigator.serviceWorker) {
        navigator.serviceWorker.getRegistrations().then(function(registrations) {
            for(let registration of registrations) {
                registration.unregister()
            }
        })
    }
</script>


<script id="scriptLoad" src="/js/allinone.min.js" async></script>












<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">
<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>




<script id="valineScript" src="//unpkg.com/valine/dist/Valine.min.js" async></script>
<script>
    document.getElementById('valineScript').addEventListener("load", function() {
        new Valine({
            el: '#comment' ,
            verify: false,
            notify: false,
            appId: '',
            appKey: '',
            placeholder: 'Just go go',
            pageSize: 10,
            avatar: 'mm',
            visitor: true
        })
    });
</script>





<script>
    document.getElementById('scriptLoad').addEventListener('load', function(){
        searchFunc("/")
    });
</script>






<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"superSample":2,"width":300,"height":600,"position":"left","hOffset":-20,"vOffset":-150},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
